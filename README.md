# coursera-uoft-neural-networks
coursera-uoft-neural-networks

This is main repository for course work as part of Coursera course for [Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks/).  I audited this course during a lag in deeplearning.ai specialization course release for Sequence Models.  I also wanted to take a step back and understand previous cited research by Andrew Ng and his "Heroes of deep learning" series.

### Instructors
Geoff Hinton - Professor Computer Science, University of Toronto

### Eureka! moment
**If you plan to only audit class, on thing to focus **
- Week 4 - Lecture 4b: A brief diversion into cognitive science discusses the reason for AI winter for neural networks triggered by Minksy's criticisms prior to rise of deep learning innovation with cloud big data and compute.  ANNs were just did not have enough resources to compete with previous state of the art in machine learning (E.g., SVMs).  ** This is key since this shows the divide between human controlled feature engineering versus end-to-end deep learning promoted by Andrew Ng, current AI industry leaders, etc.

### Key Takeaways
- Very formal, theoretical, and dense material.  However, there was a lot foundational historical context of which I was searching for (e.g., why did is take ANNs so long to reach innovation diffusion and adoption)
- GH is inspired by many cross disciplines including neurology, physics, mathematics, etc. and has carried the connectionist philosophy forward into modern times via deep learning
- GH's research leadership in backpropagation algorithms empowered DL research scale big data to learn efficiently
- GH's future interests in "Capsule Networks" are hinted strongly (5 years ago?)

### Course work

### Related
- This is foundation course to Andrew Ng's Deeplearning.ai specialization

### Future Work
- Sample key exercises and port to other languages like Python, Scala for more reinforcment
- I am still not sure how restricted Boltzmann machines (RBMs), deep belief networks, etc. are pragmatically applied today as DNNs, CNNs, and RNNs seem to dominate current DL architecture styles; I am still researching and learning more here